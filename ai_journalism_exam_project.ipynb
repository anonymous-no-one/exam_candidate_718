{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the relevant libraries below.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part I: Hazardous waste datasets - importing, merging, cleaning and the to_ton_converter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the datasets below using pandas' read_excel function: \n",
    "haz_waste = pd.read_excel(\"farlig_avfall.xlsx\") #This is the main dataset on waste per facility\n",
    "facility_1 = pd.read_excel(\"anlegg_adresse.xlsx\") #This is a dataset with address of each facility, exl closed ones\n",
    "facility_2 = pd.read_excel(\"anlegg_adresse_v2.xlsx\") #This is a dateset later aquired with closed facilities, to be able to complete the following merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vertical concatenate facility dateframes into one complete.\n",
    "facility_complete = pd.concat([facility_2, facility_1]).drop_duplicates().reset_index(drop=True) \n",
    "\n",
    "#Fill NaNs for columns \"Status\" and \"Status ansvarlig enhet\" with active for active facilties, as this column name was only included in the closed facilites dataset.\n",
    "facility_complete[[\"Status\", \"Status ansvarlig enhet\"]]=facility_complete[[\"Status\", \"Status ansvarlig enhet\"]].fillna(\"Aktiv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the waste dataset with the facility dataset\n",
    "waste_merged = pd.merge(haz_waste, facility_complete, how=\"left\", left_on=\"AnleggNummer\", right_on=\"Anleggsnr.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking columns' datatype and non-null count.\n",
    "waste_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking number of unique values in each column.\n",
    "waste_merged.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a lineplot to see overall development of hazardous waste per year.\n",
    "sns.lineplot(data=waste_merged, x=\"År\", y=\"Total mengde\", estimator=np.sum) #Estimator=np.sum important to include to get total, otherwise averarge is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatterplot with waste unit as hue to check reason for spike in 2006-08.\n",
    "sns.scatterplot(data=waste_merged, x=\"År\", y=\"Total mengde\", hue=\"Enhet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a converter to convert kilogram and liter to ton.\n",
    "def to_ton_converter(row, amount_col, unit_col, kg_to_ton=0.001, liter_to_ton=0.0008):\n",
    "    amount = row[amount_col]\n",
    "    unit = row[unit_col]\n",
    "    \n",
    "    #This checks what kind of unit it is and makes sure the right conversion value is returned.\n",
    "    if unit == 'Kilogram':\n",
    "        return amount * kg_to_ton\n",
    "    elif unit == 'Liter':\n",
    "        return amount * liter_to_ton\n",
    "    elif unit == 'Tonn':\n",
    "        return amount\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "#Defining columns to convert, and the columns to store the returned values. This are the tuples the for loop is iterating on next.\n",
    "columns_to_convert = [\n",
    "    (\"På mellomlager\", \"Mellomlager i tonn\"),\n",
    "    (\"Til gjenvinning i Norge\", \"Gjenvinning Norge i tonn\"),\n",
    "    (\"Til sluttbehandling i Norge\", \"Sluttbehandling Norge i tonn\"),\n",
    "    (\"Til gjenvinning i utlandet\", \"Gjenvinning utland i tonn\"),\n",
    "    (\"Til sluttbehandling i utlandet\", \"Sluttbehandling utland i tonn\"),\n",
    "    (\"Egenbehandlet avfall, forbrenning / pyrolyse med energi-gjenvinning\", \"Egenbeh forbrenning m energi-gjenvinning i tonn\"),\n",
    "    (\"Egenbehandlet avfall, forbrenning / pyrolyse uten energi-gjenvinning\", \"Egenbeh forbrenning u energigjenvinning i tonn\"),\n",
    "    (\"Egenbehandlet avfall, eget godkjent deponi\", \"Egenbeh eget godkjent deponi i tonn\"),\n",
    "    (\"Egenbehandlet avfall, annen behandling\", \"Egenbeh annen behandling i tonn\"),\n",
    "    (\"Total mengde\", \"Tot mengde i tonn\")\n",
    "]\n",
    "\n",
    "#Defining the unit column - it is needed to run the to_ton_converter.\n",
    "unit_col = \"Enhet\"\n",
    "\n",
    "#The for loop iterates through each column above.\n",
    "for amount_col, result_col in columns_to_convert:\n",
    "    #The apply method passes the lambda function along the rows. Rows are selected by using the axis=1 paramenter.\n",
    "    waste_merged[result_col] = waste_merged.apply(\n",
    "        #The lambda function makes sure that the to_ton_converter is run on each row individually.\n",
    "       lambda row: to_ton_converter(row, amount_col, unit_col), axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A lineplot showing total amount of waste per year after converting kilograms and liters to ton.\n",
    "sns.lineplot(data=waste_merged, x=\"År\", y=\"Tot mengde i tonn\", estimator=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a leaner dataframe and grouping it to fit the prescription drugs data.\n",
    "waste_groupby_kommune_and_year = waste_merged[[\"År\", \"Kommune\", \"Tot mengde i tonn\"]].groupby([\"Kommune\", \"År\"]).sum()\n",
    "waste_groupby_kommune_and_year.reset_index(inplace=True) #Resetting the index to keep the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the distribution of waste per \"Kommune\" per \"År\" using a histogram.\n",
    "sns.histplot(data=waste_groupby_kommune_and_year, x=\"Tot mengde i tonn\", binwidth=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part II: The prescription drugs datasets - importing, cleaning, unstacking and merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Excel files on prescription drugs to dataframes. Type 2 diabetes was a separate dataset.\n",
    "diabetes_type_II = pd.read_excel(\"diabetes_type_II.xls\", header=7)\n",
    "prescription_drugs = pd.read_excel(\"Legemiddelbrukere_ny.xls\", header=8)\n",
    "\n",
    "#Because of merged cells in the original Excel file, using ffill to fill in NaNs below with the last value above.\n",
    "diabetes_type_II.ffill(inplace=True)\n",
    "prescription_drugs.ffill(inplace=True)\n",
    "\n",
    "#Renaming some columns to make more sense of them and for easier merge later.\n",
    "diabetes_type_II.rename(columns={\"År\": \"År_snitt\", \"Geografi\": \"Kommune\"}, inplace=True)\n",
    "prescription_drugs.rename(columns={\"År\": \"År_snitt\", \"Geografi\": \"Kommune\"}, inplace=True)\n",
    "\n",
    "#Naming nameless columns - selecting them by the column number.\n",
    "diabetes_type_II.columns.values[2] = \"Type_2-diabetes\"\n",
    "prescription_drugs.columns.values[3] = \"Per_1000\"\n",
    "\n",
    "#Unstacking the main prescription drug dataset, so that each prescription drug has its own column. Then it is easier to merge with the Type 2 diabetes dataset. \n",
    "prescription_drugs_unstacked = prescription_drugs.set_index([\"År_snitt\", \"Kommune\", \"Legemiddelgruppe\"])[\"Per_1000\"].unstack()\n",
    "prescription_drugs_unstacked.reset_index(inplace=True) #Reset index to keep individual columns.\n",
    "\n",
    "#Merging the main prescription drug dataset (allergy, asthma, ADHD, diabetes) with the Type 2 diabetes dataset.\n",
    "prescription_complete = pd.merge(prescription_drugs_unstacked, diabetes_type_II, how=\"inner\", on=[\"Kommune\", \"År_snitt\"])\n",
    "\n",
    "#Prescription data is a three years rolling average and has the year range as column headers. String split used to select last year for easy merge with waste data. \n",
    "prescription_complete[\"År\"] = prescription_complete[\"År_snitt\"].str.split(\"-\").str[1]\n",
    "prescription_complete[\"År\"] = prescription_complete[\"År\"].astype(int) #assuring the values are datatype integer.\n",
    "\n",
    "#Rename some prescription drug columns for easier edibilty and handling. \n",
    "prescription_complete.rename(columns={\"Diabetesmedikamenter (A10)\": \"Diabetes\", \"Midler mot astma og KOLS (R03 unntatt R03CA)\": \"Astma_og_KOLS\", \"Allergimidler (R06A, R01AC, R01AD, R01B, S01G)\": \"Allergi\", \"ADHD-midler (C02AC02, N06BA ekskl. N06BA07)\": \"ADHD\"}, inplace=True)\n",
    "\n",
    "#Selecting the prescription drugs selected for this project into a leaner dataframe. \n",
    "prescription_to_correlate = prescription_complete[[\"År\", \"Kommune\", \"Diabetes\", \"Type_2-diabetes\", \"Astma_og_KOLS\", \"Allergi\", \"ADHD\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part III: Waste and prescription merge, and low-waste/high-waste labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging waste df and prescription df to make a new dataset ready to do a correlation analysis:\n",
    "waste_prescription = pd.merge(waste_groupby_kommune_and_year, prescription_to_correlate, how=\"inner\", on=[\"Kommune\", \"År\"])\n",
    "\n",
    "#Converting object type numeric values to float for prescription drug columns - needed to apply mathematical operations on the values.\n",
    "for col in waste_prescription.columns[3:]: #selecting column four to the end.\n",
    "    waste_prescription[col] = pd.to_numeric(waste_prescription[col], errors=\"coerce\")\n",
    "\n",
    "#Removing rows where there is no data on prescription drugs, either because of privacy or other reasons.\n",
    "waste_prescription = waste_prescription[~waste_prescription.isin(['..']).any(axis=1)]\n",
    "waste_prescription = waste_prescription[~waste_prescription.isin([':']).any(axis=1)]\n",
    "waste_prescription = waste_prescription[~waste_prescription.isnull().any(axis=1)]\n",
    "\n",
    "#Grouping by municipality to calculate the mean, then make a list of municipalities with waste > 1000 per year.\n",
    "waste_mean_kom = waste_prescription.groupby(by=\"Kommune\")[\"Tot mengde i tonn\"].mean()\n",
    "waste_mean_kom = pd.DataFrame(waste_mean_kom.reset_index()) \n",
    "above_1000_ton = waste_mean_kom.query(\"`Tot mengde i tonn`> 1000\") #querying municipalities with more than 1000 ton waste per year\n",
    "list_kom=above_1000_ton['Kommune'] #making a list of municipalities from the query aboce.\n",
    "high_waste_prescription = waste_prescription.query(\"Kommune in @list_kom\") #Using the list to create a high_waste dataframe ...\n",
    "low_waste_prescription = waste_prescription.query(\"Kommune not in @list_kom\") #... and a low_waste dataframe.\n",
    "\n",
    "#Creating a label column for low waste municipalities and high waste municipalities.\n",
    "low_waste_prescription[\"label\"]=\"low\"\n",
    "high_waste_prescription[\"label\"]=\"high\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirming the right datatypes for the dataframe.\n",
    "print(waste_prescription.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing number of unique municipalites in each dataset. \n",
    "print(low_waste_prescription[\"Kommune\"].nunique(), high_waste_prescription[\"Kommune\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part IV: Calculating the correlation coeefficient of each municipality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing pearsonr for correlation coefficient and p-value calculation from Scipy Stats.\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "#Defining a prescription_waste_correlator that can be applied to all prescription drug columns, and group the data by municipality. \n",
    "def prescription_waste_correlator(data, waste_col, group_col=\"Kommune\", year_col=\"År\"):\n",
    "    prescription_cols = [col for col in data.columns[3:8]] #Defining the columns to loop through.\n",
    "    results = pd.DataFrame() #Initializing an empty data frame.\n",
    "\n",
    "    for prescription_col in prescription_cols: #This is the for loop iterating on each column selected above.\n",
    "        #Grouping the data by Kommune (group_col) here.\n",
    "        group_data = data[[group_col, waste_col, prescription_col]].groupby([group_col])\n",
    "        \n",
    "        #Initializing lists to add municipalites, correlation coefficients and p_values when looping.\n",
    "        list_kom = []\n",
    "        list_corr = []\n",
    "        list_p_value = []\n",
    "\n",
    "        #This for loop iterates on each group (municipality) of each column.\n",
    "        for name, group in group_data: \n",
    "            #Calculating the correlation coeefficient and the p-value for each municipality. \n",
    "            corr_val, p_value = pearsonr(group[waste_col], group[prescription_col])\n",
    "            #Appending the results in each list. \n",
    "            list_kom.append(name)\n",
    "            list_corr.append(corr_val)\n",
    "            list_p_value.append(p_value)\n",
    "        \n",
    "        #Creating a dataframe to combine the lists.\n",
    "        prescription_results = pd.DataFrame({\n",
    "            group_col: list_kom,\n",
    "            \"Legemiddel\": prescription_col,\n",
    "            \"Korrelasjon\": list_corr,\n",
    "            \"P-value\": list_p_value\n",
    "        })\n",
    "        \n",
    "        #Concatenating for each run of the loop, and ignoring the pre concatenating indexes.\n",
    "        results = pd.concat([results, prescription_results], ignore_index=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the prescription_waste_correlator function on the high_waste_prescription dataframe. \n",
    "correlation_results = prescription_waste_correlator(high_waste_prescription, \"Tot mengde i tonn\")\n",
    "\n",
    "#Only select results with a p-value of < 0,05.\n",
    "p_value_0_05 = correlation_results.query(\"`P-value` < 0.05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the distribution of the correlation results using histogram.\n",
    "sns.histplot(data=correlation_results, x=\"Korrelasjon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot to show the distribution of different prescription drugs.\n",
    "ax = sns.boxplot(data=p_value_0_05, y=\"Korrelasjon\", x=\"Legemiddel\")\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=20) #Rotating the x axis a bit to increase legibility. \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part V: High waste vs low waste municipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating high_waste with low_waste to create a full dataset that now includes labels for high and low waste.\n",
    "combined_df = pd.concat([high_waste_prescription, low_waste_prescription])\n",
    "\n",
    "#Loopig through the prescription drug columns creating a lineplot for each, using the high/low label with the hue paramenter.\n",
    "for legemiddel in combined_df.columns[3:8]:\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    sns.lineplot(data=combined_df, x=\"År\", y=legemiddel, hue=\"label\", estimator=np.mean)\n",
    "    plt.title(legemiddel)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a histograph to see the allergy distribution for high vs low waste municipalities.\n",
    "sns.histplot(data=combined_df, x=\"Allergi\", hue=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part VI: Regression on a logarithmic scale to further investigate allergy findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a scatterplot to look for a pattern between allergy prescription rates (y) and hazardous waste (x)\n",
    "sns.scatterplot(data=waste_prescription, y=\"Allergi\", x=\"Tot mengde i tonn\", hue=\"År\", palette=\"plasma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=waste_prescription, y=\"Allergi\", x=\"Tot mengde i tonn\", hue=\"År\", palette=\"plasma\")\n",
    "plt.xscale(\"log\") #Adding a logarithmic scale to the x axis (waste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also importing spearmanr for rank correlation as my dataset is skewed.\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "prescription = \"Allergi\" #defining prescription as a variable to easily switch between different prescription drugs.\n",
    "correlator = pearsonr #defining correlator as a variable to easily switch between pearsonr and spearmanr.\n",
    "\n",
    "#Creating logarithmic columns for waste and prescription.\n",
    "waste_prescription[\"log_waste\"] = np.log10(waste_prescription[\"Tot mengde i tonn\"]+1)\n",
    "waste_prescription[\"log\"+prescription] = np.log10(waste_prescription[prescription]+1)\n",
    "\n",
    "#Referencing the lmplot I want to run in the variable g.\n",
    "g = sns.lmplot(data=waste_prescription, y=\"log\"+prescription, x=\"log_waste\")\n",
    "\n",
    "#Defining a function that runs pearsonr, or spearmanr, and annotate the values to a plot. This is stolen from Stack Overflow.\n",
    "def annotate(data, **kws):\n",
    "    r, p = correlator(data[\"log\"+prescription], data['log_waste']) #here I am running pearsonr, or spearmanr, on the data\n",
    "    ax = plt.gca()\n",
    "    ax.text(.8, .05, \"r={:.2f}, p={:.2g}\".format(r, p), #defining the placement of the annotation, down to the right.\n",
    "            transform=ax.transAxes)\n",
    "g.map_dataframe(annotate) #adding the annotation function to the plot.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding hue=\"År\" to the lineplot to visualize regression per year.\n",
    "sns.lmplot(data=waste_prescription, y=\"log\"+prescription, x=\"log_waste\", hue=\"År\", palette=\"plasma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part VII: Correlation per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a variable for unique years in the year column.\n",
    "years = waste_prescription[\"År\"].unique()\n",
    "\n",
    "#Creating a dictionary to store the results for each iteration of the loop.\n",
    "results = {\n",
    "    \"year\": [], \n",
    "    \"corr\": [], \n",
    "    \"p_value\": []\n",
    "    }\n",
    "\n",
    "#A for loop that iterates on each year.\n",
    "for year in years:\n",
    "    #The data is first filtered for the year the loop is iterating on. \n",
    "    waste_prescription_years = waste_prescription[waste_prescription[\"År\"] == year]\n",
    "    \n",
    "    #Then it is calculating the correlation (Pearson's or Spearman's depending on input above) and p-value for that year.\n",
    "    corr, p_value = correlator(waste_prescription_years[\"Allergi\"], waste_prescription_years[\"log_waste\"])\n",
    "    \n",
    "    #The results are appended in the dictionary defined above.\n",
    "    results[\"year\"].append(year)\n",
    "    results[\"corr\"].append(corr)\n",
    "    results[\"p_value\"].append(p_value)\n",
    "\n",
    "#The dictionary is converted into a data frame.\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "#Sorting the results by years.\n",
    "results_df.sort_values(by=\"year\", inplace=True)\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=results_df, x=\"year\", y=\"corr\") #a lineplot of correlation over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part VIII: Creating a 3D scatter plot using plotly and with the help of ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Importing the libraries needed to create the 3D plot.\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "plot_data = waste_prescription\n",
    "\n",
    "#Define the differenct axes of the 3D plot\n",
    "x_column = \"log_waste\"\n",
    "y_column = \"År\"\n",
    "z_column = \"logAllergi\"\n",
    "\n",
    "#Preparing the data for linear regression fitting\n",
    "X = plot_data[[x_column, y_column]]\n",
    "y = plot_data[z_column]\n",
    "\n",
    "#Fitting the linear regression model on the data\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Creating an array for predictions for the regression line\n",
    "X_pred = pd.DataFrame({\n",
    "    x_column: np.linspace(X[x_column].min(), X[x_column].max(), 100),\n",
    "    y_column: np.linspace(X[y_column].min(), X[y_column].max(), 100)\n",
    "})\n",
    "y_pred = model.predict(X_pred)\n",
    "\n",
    "# Creating the 3D figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add scatter plot - waste data\n",
    "fig.add_trace(go.Scatter3d(x=plot_data[x_column], y=plot_data[y_column], z=plot_data[z_column],\n",
    "                           mode='markers', marker=dict(size=2, color='blue',)),)\n",
    "\n",
    "# Add line plot - prediction data, as defined above\n",
    "fig.add_trace(go.Scatter3d(x=X_pred[x_column], y=X_pred[y_column], z=y_pred,\n",
    "                           mode='lines', line=dict(color='red', width=2)))\n",
    "\n",
    "# Updatingn the plot appearance\n",
    "fig.update_layout(title='3D Regression Line Plot',\n",
    "                  scene=dict(\n",
    "                      xaxis_title=x_column,\n",
    "                      yaxis_title=y_column,\n",
    "                      zaxis_title=z_column))\n",
    "\n",
    "# And this is showing the figure.\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: Let's see if I can do a Principal Component Analysis (PCA) on the data - ChatGPT has been helpful also here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries needed rom sklearn, StandardScaler to standardize the data,  PCA to run the PCA.\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#grouping by municipality to get on PCA value per municipality.\n",
    "new_pca = waste_prescription.groupby(\"Kommune\").mean().reset_index()\n",
    "\n",
    "#selecting the numerical values that goes into the PCA, and standardize it with the StandardScaler.\n",
    "x = new_pca.iloc[:,3:-1].values\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "pca = PCA(n_components=3) #defining number of components for PCA.\n",
    "principal_components = pca.fit_transform(x) #running the pca on the standardized data.\n",
    "principal_df = pd.DataFrame(data=principal_components, columns=[\"PC_1\", \"PC_2\", \"PC_3\"]) #Creating a data frame of the results.\n",
    "\n",
    "#concatenating pca result data frame with original df on rows so that PCA values are alligned with the right municipality.\n",
    "final_df = pd.concat([new_pca, principal_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a 3D scatter plot to display PC_1, PC_2 and PC_3.\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "#Defining the data for the scatter plot.\n",
    "sc = ax.scatter(final_df[\"PC_1\"], final_df[\"PC_2\"], final_df[\"PC_3\"], c='r', marker='o')\n",
    "\n",
    "#Annotate each point with the municipality name (if applicable)\n",
    "for i, municipality in enumerate(final_df[\"Kommune\"]):\n",
    "    ax.text(final_df.loc[i, \"PC_1\"], final_df.loc[i, \"PC_2\"], final_df.loc[i, \"PC_3\"], municipality)\n",
    "\n",
    "#Set labels and plot title.\n",
    "ax.set_xlabel(\"PC_1\")\n",
    "ax.set_ylabel(\"PC_2\")\n",
    "ax.set_zlabel(\"PC_3\")\n",
    "ax.set_title('PCA: Hazardous waste and prescription drugs')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I want to see the weights the different input in the PCA has on the different principal components. \n",
    "df_selected = new_pca.iloc[:,3:-1] #defining a variable for the columns used in the PCA.\n",
    "\n",
    "#Extract loadings/weights as loadings\n",
    "loadings = pca.components_\n",
    "\n",
    "#Create a data frame for loadings, setting each PC as columns and the original columns, that went into the PCA, as rows.\n",
    "loadings_df = pd.DataFrame(loadings.T, columns=['PC_1', 'PC_2', 'PC_3'], index=df_selected.columns)\n",
    "print(loadings_df)\n",
    "\n",
    "# Plot loadings for each principal component\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 6))\n",
    "\n",
    "for i, ax in enumerate(axes): #for loop to create a bar plot for each PC.\n",
    "    loadings_df.iloc[:, i].plot(kind='bar', ax=ax)\n",
    "    ax.set_title(f'Loadings for Principal Component {i+1}')\n",
    "    ax.set_ylabel('Loading Score')\n",
    "    ax.set_xlabel('Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show() #Display all three plots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oslomet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
